#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é›†æˆæµ‹è¯•æ–‡ä»¶ - æµ‹è¯•å®Œæ•´çš„DITAè½¬æ¢æµç¨‹
ä»ç¬¬ä¸€å±‚è¾“å…¥PDFå’ŒWordæ–‡ä»¶ï¼Œç„¶åä¿å­˜æ¯ä¸€å±‚çš„è¾“å‡º
"""

import sys
import json
from pathlib import Path
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# å¯¼å…¥å„å±‚æ¨¡å—
from src.layer1_preprocessing import PDFProcessor, WordProcessor
from src.layer2_semantic import DocumentAnalyzer
from src.layer3_dita_conversion import DITAConverter
from src.layer4_quality_assurance import QAManager
from src.utils.logger import setup_logger


def ensure_output_dir(output_dir: Path):
    """ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨å¹¶è¿”å›ç›®å½•è·¯å¾„"""
    output_dir.mkdir(parents=True, exist_ok=True)
    return output_dir


def save_layer_output(output_dir: Path, filename: str, content: str):
    """ä¿å­˜å±‚è¾“å‡ºåˆ°æ–‡ä»¶"""
    output_path = output_dir / filename
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(content)
    return output_path


def save_layer_result(output_dir: Path, filename: str, result: dict):
    """ä¿å­˜å±‚ç»“æœï¼ˆJSONæ ¼å¼ï¼‰åˆ°æ–‡ä»¶"""
    output_path = output_dir / filename
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    return output_path


def process_file(file_path: Path, output_root: Path, use_ai: bool = True):
    """å¤„ç†å•ä¸ªæ–‡ä»¶çš„å®Œæ•´DITAè½¬æ¢æµç¨‹"""
    print(f"\n" + "="*100)
    print(f"ğŸ” å¼€å§‹å¤„ç†æ–‡ä»¶: {file_path.name}")
    print(f"ğŸ“ æ–‡ä»¶è·¯å¾„: {file_path}")
    print(f"ğŸ“… å¤„ç†æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*100)

    # ä¸ºæ¯ä¸ªæ–‡ä»¶åˆ›å»ºç‹¬ç«‹çš„è¾“å‡ºç›®å½•
    file_output_dir = output_root / file_path.stem
    ensure_output_dir(file_output_dir)

    # ========== ç¬¬ä¸€å±‚: é¢„å¤„ç† ==========
    print("\n" + "="*70)
    print("ğŸ§ª ç¬¬ä¸€å±‚: æ–‡æ¡£é¢„å¤„ç†")
    print("="*70)

    layer1_output_dir = file_output_dir / "layer1"
    ensure_output_dir(layer1_output_dir)

    layer1_result = None
    file_extension = file_path.suffix.lower()

    if file_extension == '.pdf':
        # å¤„ç†PDFæ–‡ä»¶
        print(f"ğŸ“„ æ­£åœ¨å¤„ç†PDFæ–‡ä»¶...")
        processor = PDFProcessor(use_marker=True, use_ocr=True)
        layer1_result = processor.process(file_path)
    elif file_extension in ['.docx', '.doc']:
        # å¤„ç†Wordæ–‡ä»¶
        print(f"ğŸ“„ æ­£åœ¨å¤„ç†Wordæ–‡ä»¶...")
        processor = WordProcessor()
        layer1_result = processor.process(file_path)
    else:
        print(f"âŒ ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_extension}")
        return False

    if not layer1_result['success']:
        print(f"âŒ ç¬¬ä¸€å±‚é¢„å¤„ç†å¤±è´¥: {layer1_result.get('error')}")
        return False

    # ä¿å­˜ç¬¬ä¸€å±‚è¾“å‡º
    save_layer_result(layer1_output_dir, f"layer1_result.json", layer1_result)
    save_layer_output(layer1_output_dir, f"layer1_markdown.txt", layer1_result['markdown'])

    print(f"âœ… ç¬¬ä¸€å±‚é¢„å¤„ç†æˆåŠŸ!")
    # å¤„ç†ä¸åŒå¤„ç†å™¨å¯èƒ½æ²¡æœ‰'method'é”®çš„æƒ…å†µ
    method = layer1_result['metadata'].get('method', 'unknown')
    print(f"   æå–æ–¹æ³•: {method}")
    print(f"   è¾“å‡ºå·²ä¿å­˜åˆ°: {layer1_output_dir}")

    # ========== ç¬¬äºŒå±‚: è¯­ä¹‰åˆ†æ ==========
    print("\n" + "="*70)
    print("ğŸ§ª ç¬¬äºŒå±‚: è¯­ä¹‰åˆ†æ")
    print("="*70)

    layer2_output_dir = file_output_dir / "layer2"
    ensure_output_dir(layer2_output_dir)

    analyzer = DocumentAnalyzer(use_ai=use_ai)
    layer2_result = analyzer.analyze(layer1_result['markdown'])

    # ä¿å­˜ç¬¬äºŒå±‚è¾“å‡º
    save_layer_result(layer2_output_dir, f"layer2_result.json", layer2_result)
    save_layer_output(layer2_output_dir, f"layer2_chunks.txt", 
                     "\n\n---\n\n".join([chunk['title'] + "\n" + chunk['content'] for chunk in layer2_result['chunks']]))

    print(f"âœ… ç¬¬äºŒå±‚è¯­ä¹‰åˆ†ææˆåŠŸ!")
    print(f"   æ€»å—æ•°: {layer2_result['statistics']['total_chunks']}")
    print(f"   ç±»å‹åˆ†å¸ƒ: {layer2_result['statistics']['type_distribution']}")
    print(f"   è¾“å‡ºå·²ä¿å­˜åˆ°: {layer2_output_dir}")

    # ========== ç¬¬ä¸‰å±‚: DITAè½¬æ¢ ==========
    print("\n" + "="*70)
    print("ğŸ§ª ç¬¬ä¸‰å±‚: DITAè½¬æ¢")
    print("="*70)

    layer3_output_dir = file_output_dir / "layer3"
    ensure_output_dir(layer3_output_dir)

    converter = DITAConverter(use_ai=use_ai, max_fix_iterations=3)

    # ç¡®å®šæ–‡æ¡£ç±»å‹ï¼ˆä½¿ç”¨æœ€ä¸»è¦çš„ç±»å‹ï¼‰
    type_dist = layer2_result['statistics']['type_distribution']
    if type_dist:
        primary_type = max(type_dist.items(), key=lambda x: x[1])[0]
    else:
        primary_type = "Concept"  # é»˜è®¤ç±»å‹

    print(f"ğŸ“‹ ç¡®å®šæ–‡æ¡£ç±»å‹: {primary_type}")

    # å‡†å¤‡æ‰¹æ¬¡è½¬æ¢ - ä»å±‚2è·å–chunks
    chunks = layer2_result.get('chunks', [])
    
    # å¦‚æœæ²¡æœ‰chunksï¼Œä½¿ç”¨åŸå§‹å†…å®¹ä½œä¸ºå•ä¸ªchunk
    if not chunks:
        chunks = [{
            'id': 'single_chunk',
            'content': layer1_result['markdown'],
            'title': layer2_result.get('title', 'Untitled Document'),
            'type': primary_type
        }]
    else:
        # ç¡®ä¿æ¯ä¸ªchunkéƒ½æœ‰typeå­—æ®µ
        for chunk in chunks:
            if 'type' not in chunk and 'classification' in chunk:
                chunk['type'] = chunk['classification']['type']
            elif 'type' not in chunk:
                chunk['type'] = primary_type

    print(f"ğŸ“‹ å‡†å¤‡è½¬æ¢ {len(chunks)} ä¸ªå—")

    # æ‰¹é‡è½¬æ¢ä¸ºDITA
    layer3_result = converter.convert_batch(chunks, output_dir=layer3_output_dir)

    if layer3_result['failed'] > 0:
        print(f"âš ï¸  ç¬¬ä¸‰å±‚DITAè½¬æ¢éƒ¨åˆ†å¤±è´¥: {layer3_result['failed']} ä¸ªå—å¤±è´¥")
    else:
        print(f"âœ… ç¬¬ä¸‰å±‚DITAæ‰¹é‡è½¬æ¢æˆåŠŸ!")

    print(f"   æ€»æ•°: {layer3_result['total']}")
    print(f"   æˆåŠŸ: {layer3_result['success']}")
    print(f"   å¤±è´¥: {layer3_result['failed']}")
    print(f"   æˆåŠŸç‡: {layer3_result['success_rate']:.1%}")
    print(f"   è¾“å‡ºå·²ä¿å­˜åˆ°: {layer3_output_dir}")

    # ä¿å­˜ç¬¬ä¸‰å±‚è¾“å‡º
    save_layer_result(layer3_output_dir, f"layer3_result.json", layer3_result)

    # ========== ç¬¬å››å±‚: è´¨é‡ä¿è¯ ==========  
    print("\n" + "="*70)
    print("ç¬¬å››å±‚: è´¨é‡ä¿è¯")
    layer4_output_dir = ensure_output_dir(file_output_dir / "layer4")

    qa_manager = QAManager(
        use_dita_ot=False,        # ä¸ä½¿ç”¨DITA-OTï¼ˆéœ€è¦å•ç‹¬å®‰è£…ï¼‰
        use_ai_repair=use_ai,     # ä½¿ç”¨AIä¿®å¤
        max_iterations=3          # æœ€å¤§è¿­ä»£3æ¬¡
    )

    # å‡†å¤‡æ‰¹é‡å¤„ç†çš„DITAæ–‡æ¡£åˆ—è¡¨
    dita_documents = []
    for i, result in enumerate(layer3_result['results'], 1):
        if not result['success']:
            continue
        
        # è·å–æ–‡æ¡£è·¯å¾„
        content_type = result['content_type']
        title = result['title']
        safe_title = "".join(c if c.isalnum() else '_' for c in title)[:50]
        filename = f"{i:03d}_{content_type.lower()}_{safe_title}.dita"
        dita_file_path = layer3_output_dir / filename
        
        # è¯»å–DITAæ–‡ä»¶
        try:
            with open(dita_file_path, 'r', encoding='utf-8') as f:
                dita_xml = f.read()
        except Exception as e:
            print(f"âŒ è¯»å–DITAæ–‡ä»¶å¤±è´¥: {e}")
            continue
        
        dita_documents.append({
            'xml': dita_xml,
            'type': content_type,
            'metadata': {
                'layer1_confidence': layer1_result.get('confidence', 0.0),
                'layer2_confidence': layer2_result['statistics']['overall_avg_confidence'],
                'layer3_iterations': result['metadata']['iterations'],
                'title': title,
                'filename': filename
            }
        })

    # ä½¿ç”¨æ‰¹é‡å¤„ç†æ–¹æ³•å¤„ç†æ–‡æ¡£
    if dita_documents:
        layer4_result = qa_manager.process_batch(dita_documents, output_dir=layer4_output_dir)
        
        print(f"\nâœ… ç¬¬å››å±‚è´¨é‡ä¿è¯å®Œæˆ!")
        print(f"   å¤„ç†æ–‡æ¡£æ•°: {layer4_result['total']}")
        print(f"   æˆåŠŸæ•°: {layer4_result['success']}")
        print(f"   å¤±è´¥æ•°: {layer4_result['failed']}")
        print(f"   æˆåŠŸç‡: {layer4_result['success_rate']:.1%}")
        print(f"   è¾“å‡ºå·²ä¿å­˜åˆ°: {layer4_output_dir}")
        
        # ä¿å­˜æ€»ä½“è´¨é‡ä¿è¯æŠ¥å‘Š
        save_layer_result(layer4_output_dir, f"layer4_overall_result.json", layer4_result)
        
        # å¦‚æœç”Ÿæˆäº†åˆå¹¶æ–‡æ¡£ï¼Œè®°å½•ä¿¡æ¯
        if 'merged_document_path' in layer4_result:
            print(f"\nğŸ“š å·²ç”Ÿæˆåˆå¹¶åçš„å®Œæ•´æ–‡æ¡£:")
            print(f"   æ–‡ä»¶è·¯å¾„: {layer4_result['merged_document_path']}")
            print(f"   è´¨é‡çŠ¶æ€: {layer4_result['merged_document_result']['quality_report']['overall_status']}")
    else:
        print(f"\nâš ï¸ æ²¡æœ‰å¯å¤„ç†çš„DITAæ–‡æ¡£")

    print(f"\n" + "="*100)
    print(f"ğŸ‰ æ–‡ä»¶å¤„ç†å®Œæˆ: {file_path.name}")
    print(f"ğŸ’¾ æ‰€æœ‰è¾“å‡ºå·²ä¿å­˜åˆ°: {file_output_dir}")
    print("="*100)
    return True


def main():
    """ä¸»å‡½æ•°"""
    # è®¾ç½®æ—¥å¿—
    setup_logger("integration_test")

    print("\n" + "="*100)
    print("ğŸ¯ DITAè½¬æ¢å™¨é›†æˆæµ‹è¯•")
    print("="*100)
    print("æ­¤æµ‹è¯•å°†å¤„ç†PDFå’ŒWordæ–‡ä»¶ï¼Œå¹¶ä¿å­˜æ¯ä¸€å±‚çš„è¾“å‡º")
    print("æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: .pdf, .docx, .doc")
    print("="*100)

    # è§£æå‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) < 2:
        print("\nâŒ ç”¨æ³•: python test_integration.py <æ–‡ä»¶1> [<æ–‡ä»¶2> ...]")
        print("ç¤ºä¾‹: python test_integration.py data/input/sample.pdf data/input/sample.docx")
        sys.exit(1)

    # è·å–è¾“å…¥æ–‡ä»¶åˆ—è¡¨
    input_files = []
    for path_str in sys.argv[1:]:
        path = Path(path_str)
        if not path.exists():
            print(f"\nâŒ æ–‡ä»¶ä¸å­˜åœ¨: {path_str}")
            continue
        if not path.is_file():
            print(f"\nâŒ ä¸æ˜¯æ–‡ä»¶: {path_str}")
            continue
        input_files.append(path)

    if not input_files:
        print("\nâŒ æ²¡æœ‰æœ‰æ•ˆçš„è¾“å…¥æ–‡ä»¶")
        sys.exit(1)

    # åˆ›å»ºè¾“å‡ºæ ¹ç›®å½•
    output_root = project_root / "data" / "output" / "integration_test"
    ensure_output_dir(output_root)
    print(f"\nğŸ“ è¾“å‡ºæ ¹ç›®å½•: {output_root}")

    # è¯¢é—®æ˜¯å¦ä½¿ç”¨AIåŠŸèƒ½
    use_ai = True
    if len(input_files) > 1:
        response = input("\nğŸ’¡ æ˜¯å¦ä½¿ç”¨AIåŠŸèƒ½ï¼Ÿ(y/nï¼Œé»˜è®¤y): ").lower().strip()
        if response == 'n':
            use_ai = False

    # å¤„ç†æ‰€æœ‰è¾“å…¥æ–‡ä»¶
    print(f"\nğŸ”„ ä½¿ç”¨AIåŠŸèƒ½: {'æ˜¯' if use_ai else 'å¦'}")
    print(f"ğŸ“‹ å¾…å¤„ç†æ–‡ä»¶æ•°: {len(input_files)}")
    print("\n" + "="*70)

    success_count = 0
    for i, file_path in enumerate(input_files, 1):
        print(f"\n{'-'*70}")
        print(f"ğŸ“„ æ–‡ä»¶ {i}/{len(input_files)}: {file_path.name}")
        print(f"{'-'*70}")
        
        if process_file(file_path, output_root, use_ai):
            success_count += 1

    # æ˜¾ç¤ºå¤„ç†ç»“æœ
    print(f"\n" + "="*100)
    print(f"ğŸ“Š å¤„ç†ç»“æœç»Ÿè®¡")
    print("="*100)
    print(f"æ€»æ–‡ä»¶æ•°: {len(input_files)}")
    print(f"æˆåŠŸå¤„ç†: {success_count}")
    print(f"å¤±è´¥å¤„ç†: {len(input_files) - success_count}")
    print(f"æˆåŠŸç‡: {success_count / len(input_files) * 100:.1f}%")
    print(f"æ‰€æœ‰è¾“å‡ºå·²ä¿å­˜åˆ°: {output_root}")
    print("="*100)

    if success_count == len(input_files):
        print("ğŸ‰ æ‰€æœ‰æ–‡ä»¶å¤„ç†æˆåŠŸ!")
        return True
    else:
        print("âš ï¸  éƒ¨åˆ†æ–‡ä»¶å¤„ç†å¤±è´¥ï¼Œè¯·æŸ¥çœ‹æ—¥å¿—")
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
