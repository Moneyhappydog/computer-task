"""
æ–‡æ¡£åˆ†æå™¨ - Layer 2æ ¸å¿ƒæ¨¡å—
æ•´åˆNLPç‰¹å¾æå–å’Œä¸‰å±‚åˆ†ç±»å™¨ï¼Œå®ç°å®Œæ•´çš„è¯­ä¹‰ç†è§£æµç¨‹
"""
from pathlib import Path
from typing import Dict, List
import json
import re

from .nlp_features import NLPFeatureExtractor, extract_structural_features
from .classifiers.hybrid_classifier import HybridClassifier
from .active_learning import ActiveLearningManager

# å¯¼å…¥å·¥å…·æ¨¡å—
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.utils.logger import setup_logger

logger = setup_logger('document_analyzer')

class DocumentAnalyzer:
    """
    æ–‡æ¡£è¯­ä¹‰åˆ†æå™¨ - Layer 2æ ¸å¿ƒ
    
    æµç¨‹ï¼š
    1. æ–‡æœ¬åˆ†å—ï¼ˆæŒ‰H2/H3æ ‡é¢˜ï¼‰
    2. ç‰¹å¾æå–ï¼ˆNLP + ç»“æ„åŒ–ç‰¹å¾ï¼‰
    3. ä¸‰å±‚åˆ†ç±»ï¼ˆè§„åˆ™ â†’ LLM â†’ äººå·¥å®¡æ ¸ï¼‰
    4. ç»“æœèåˆï¼ˆç½®ä¿¡åº¦åŠ æƒï¼‰
    """
    
    def __init__(self, use_ai: bool = True, chunk_size: int = 500):
        """
        åˆå§‹åŒ–æ–‡æ¡£åˆ†æå™¨
        
        Args:
            use_ai: æ˜¯å¦ä½¿ç”¨AIåˆ†ç±»å™¨
            chunk_size: åˆ†å—å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼Œç”¨äºå¤‡ç”¨åˆ†å—ç­–ç•¥ï¼‰
        """
        logger.info("ğŸ§  åˆå§‹åŒ–æ–‡æ¡£åˆ†æå™¨...")
        
        self.chunk_size = chunk_size
        self.use_ai = use_ai
        
        # åˆå§‹åŒ–NLPç‰¹å¾æå–å™¨
        self.nlp_extractor = NLPFeatureExtractor()
        
        # åˆå§‹åŒ–æ··åˆåˆ†ç±»å™¨
        self.classifier = HybridClassifier(use_ai=use_ai)
        
        # åˆå§‹åŒ–ä¸»åŠ¨å­¦ä¹ ç®¡ç†å™¨
        self.active_learning = ActiveLearningManager()
        
        logger.info("âœ… æ–‡æ¡£åˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def analyze(self, markdown_content: str, metadata: Dict = None) -> Dict:
        """
        åˆ†æMarkdownæ–‡æ¡£
        
        Args:
            markdown_content: Markdownå†…å®¹
            metadata: Layer 1çš„å…ƒæ•°æ®ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            åˆ†æç»“æœ:
            {
                "metadata": {...},
                "chunks": [
                    {
                        "id": "chunk_1",
                        "title": "Installing Python",
                        "level": 2,
                        "content": "...",
                        "features": {...},
                        "classification": {
                            "type": "Task",
                            "confidence": 0.95,
                            "scores": {...}
                        }
                    }
                ],
                "statistics": {...}
            }
        """
        logger.info("=" * 70)
        logger.info("ğŸ“Š å¼€å§‹è¯­ä¹‰åˆ†æ...")
        logger.info("=" * 70)
        
        # Step 1: æ–‡æœ¬åˆ†å—
        logger.info("\n[Step 1/3] æ–‡æœ¬åˆ†å—...")
        chunks = self._chunk_by_headings(markdown_content)
        logger.info(f"  âœ“ åˆ†å—å®Œæˆï¼š{len(chunks)} ä¸ªè¯­ä¹‰å—")
        
        # Step 2 & 3: ç‰¹å¾æå– + åˆ†ç±»
        logger.info("\n[Step 2/3] ç‰¹å¾æå– + åˆ†ç±»...")
        analyzed_chunks = []
        
        for i, chunk in enumerate(chunks, 1):
            logger.info(f"\n  [{i}/{len(chunks)}] å¤„ç†: {chunk['title'][:50]}...")
            
            # æå–ç‰¹å¾
            features = self._extract_features(chunk)
            logger.info(f"    âœ“ ç‰¹å¾æå–å®Œæˆ")
            
            # åˆ†ç±»
            classification = self.classifier.classify(chunk, features)
            logger.info(
                f"    âœ“ åˆ†ç±»: {classification['type']} "
                f"(ç½®ä¿¡åº¦: {classification['confidence']:.2f})"
            )
            
            analyzed_chunks.append({
                **chunk,
                "features": features,
                "classification": classification
            })
        
        # Step 4: è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        logger.info("\n[Step 3/3] è®¡ç®—ç»Ÿè®¡ä¿¡æ¯...")
        statistics = self._compute_statistics(analyzed_chunks)
        
        logger.info("=" * 70)
        logger.info("âœ… è¯­ä¹‰åˆ†æå®Œæˆï¼")
        logger.info(f"  æ€»å—æ•°: {statistics['total_chunks']}")
        logger.info(f"  ç±»å‹åˆ†å¸ƒ: {statistics['type_distribution']}")
        logger.info(f"  å¹³å‡ç½®ä¿¡åº¦: {statistics['overall_avg_confidence']:.2f}")
        logger.info("=" * 70)
        
        return {
            "metadata": metadata or {},
            "chunks": analyzed_chunks,
            "statistics": statistics
        }
    
    def _chunk_by_headings(self, content: str) -> List[Dict]:
        """
        æŒ‰æ ‡é¢˜åˆ†å—
        
        è§„åˆ™ï¼š
        - H2 (##) å’Œ H3 (###) ä½œä¸ºåˆ†å—è¾¹ç•Œ
        - æ¯ä¸ªæ ‡é¢˜åŠå…¶ä¸‹å±å†…å®¹ä½œä¸ºä¸€ä¸ªè¯­ä¹‰å—
        
        Args:
            content: Markdownå†…å®¹
            
        Returns:
            åˆ†å—åˆ—è¡¨
        """
        # æŒ‰H2å’ŒH3æ ‡é¢˜åˆ†å‰²
        pattern = r'^(#{2,3})\s+(.+)$'
        lines = content.split('\n')
        
        chunks = []
        current_chunk = None
        
        for line in lines:
            match = re.match(pattern, line)
            
            if match:
                # ä¿å­˜ä¸Šä¸€ä¸ªå—
                if current_chunk and current_chunk["content"].strip():
                    chunks.append(current_chunk)
                
                # å¼€å§‹æ–°å—
                level = len(match.group(1))
                title = match.group(2).strip()
                current_chunk = {
                    "id": f"chunk_{len(chunks) + 1}",
                    "title": title,
                    "level": level,
                    "content": ""
                }
            elif current_chunk is not None:
                current_chunk["content"] += line + "\n"
        
        # æ·»åŠ æœ€åä¸€ä¸ªå—
        if current_chunk and current_chunk["content"].strip():
            chunks.append(current_chunk)
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ ‡é¢˜ï¼Œå°è¯•æŒ‰å­—ç¬¦æ•°åˆ†å—
        if not chunks and content.strip():
            logger.warning("  âš ï¸ æœªæ‰¾åˆ°H2/H3æ ‡é¢˜ï¼Œä½¿ç”¨å­—ç¬¦æ•°åˆ†å—")
            chunks = self._chunk_by_size(content)
        
        return chunks
    
    def _chunk_by_size(self, content: str) -> List[Dict]:
        """æŒ‰å­—ç¬¦æ•°åˆ†å—ï¼ˆå¤‡ç”¨ç­–ç•¥ï¼‰"""
        chunks = []
        lines = content.split('\n')
        current_chunk = ""
        chunk_id = 1
        
        for line in lines:
            if len(current_chunk) + len(line) > self.chunk_size:
                if current_chunk.strip():
                    chunks.append({
                        "id": f"chunk_{chunk_id}",
                        "title": f"Section {chunk_id}",
                        "level": 2,
                        "content": current_chunk
                    })
                    chunk_id += 1
                current_chunk = line + "\n"
            else:
                current_chunk += line + "\n"
        
        # æ·»åŠ æœ€åä¸€å—
        if current_chunk.strip():
            chunks.append({
                "id": f"chunk_{chunk_id}",
                "title": f"Section {chunk_id}",
                "level": 2,
                "content": current_chunk
            })
        
        return chunks
    
    def _extract_features(self, chunk: Dict) -> Dict:
        """
        æå–å®Œæ•´ç‰¹å¾
        
        Args:
            chunk: æ–‡æœ¬å—
            
        Returns:
            ç‰¹å¾å­—å…¸ï¼ˆNLPç‰¹å¾ + ç»“æ„åŒ–ç‰¹å¾ï¼‰
        """
        content = chunk["content"]
        
        # NLPç‰¹å¾ï¼ˆè¯æ€§ã€ä¾å­˜ã€å®ä½“ç­‰ï¼‰
        nlp_features = self.nlp_extractor.extract_all_features(content)
        
        # ç»“æ„åŒ–ç‰¹å¾ï¼ˆåˆ—è¡¨ã€è¡¨æ ¼ã€ä»£ç å—ç­‰ï¼‰
        structural_features = extract_structural_features(content)
        
        # åˆå¹¶
        return {
            **nlp_features,
            **structural_features,
            "title": chunk["title"],
            "level": chunk["level"]
        }
    
    def _compute_statistics(self, chunks: List[Dict]) -> Dict:
        """è®¡ç®—ç»Ÿè®¡ä¿¡æ¯"""
        type_counts = {}
        confidence_sum = {}
        confidence_count = {}
        needs_review_count = 0
        
        for chunk in chunks:
            classification = chunk["classification"]
            ctype = classification["type"]
            conf = classification["confidence"]
            
            # ç»Ÿè®¡ç±»å‹åˆ†å¸ƒ
            type_counts[ctype] = type_counts.get(ctype, 0) + 1
            
            # ç´¯è®¡ç½®ä¿¡åº¦ï¼ˆæ’é™¤needs_reviewï¼‰
            if ctype != "needs_review":
                confidence_sum[ctype] = confidence_sum.get(ctype, 0) + conf
                confidence_count[ctype] = confidence_count.get(ctype, 0) + 1
            else:
                needs_review_count += 1
        
        # è®¡ç®—å¹³å‡ç½®ä¿¡åº¦
        avg_confidence = {
            ctype: confidence_sum[ctype] / confidence_count[ctype]
            for ctype in confidence_sum
        }
        
        # æ€»ä½“å¹³å‡ç½®ä¿¡åº¦
        total_conf = sum(confidence_sum.values())
        total_count = sum(confidence_count.values())
        overall_avg = total_conf / total_count if total_count > 0 else 0.0
        
        return {
            "total_chunks": len(chunks),
            "type_distribution": type_counts,
            "average_confidence": avg_confidence,
            "overall_avg_confidence": overall_avg,
            "needs_review": needs_review_count
        }
    
    def save_results(self, results: Dict, output_path: Path):
        """
        ä¿å­˜åˆ†æç»“æœ
        
        Args:
            results: åˆ†æç»“æœ
            output_path: è¾“å‡ºè·¯å¾„
        """
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ğŸ’¾ åˆ†æç»“æœå·²ä¿å­˜: {output_path}")