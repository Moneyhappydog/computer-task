"""
ç»“æœèåˆå¼•æ“
å®ç°ç½®ä¿¡åº¦åŠ æƒå¹³å‡èåˆ Tier 1 å’Œ Tier 2 çš„åˆ†ç±»ç»“æœ
"""
from typing import Dict, List

# å¯¼å…¥å·¥å…·æ¨¡å—
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from src.utils.logger import setup_logger

logger = setup_logger('fusion_engine')

class FusionEngine:
    """å¤šåˆ†ç±»å™¨ç»“æœèåˆå¼•æ“"""
    
    def __init__(
        self,
        tier1_weight: float = 0.3,
        tier2_weight: float = 0.7,
        confidence_threshold: float = 0.6
    ):
        """
        åˆå§‹åŒ–èåˆå¼•æ“
        
        Args:
            tier1_weight: Tier 1ï¼ˆè§„åˆ™ï¼‰æƒé‡
            tier2_weight: Tier 2ï¼ˆLLMï¼‰æƒé‡
            confidence_threshold: ç½®ä¿¡åº¦é˜ˆå€¼ï¼ˆä½äºæ­¤å€¼è§¦å‘Tier 3äººå·¥å®¡æ ¸ï¼‰
        """
        self.weights = {
            "tier1": tier1_weight,
            "tier2": tier2_weight
        }
        self.confidence_threshold = confidence_threshold
        
        # éªŒè¯æƒé‡å’Œä¸º1
        total_weight = sum(self.weights.values())
        if abs(total_weight - 1.0) > 0.01:
            raise ValueError(f"æƒé‡å’Œå¿…é¡»ä¸º1ï¼Œå½“å‰ä¸º {total_weight}")
        
        logger.info(
            f"ğŸ”€ èåˆå¼•æ“åˆå§‹åŒ–: "
            f"Tier1={tier1_weight:.1f}, "
            f"Tier2={tier2_weight:.1f}, "
            f"é˜ˆå€¼={confidence_threshold:.1f}"
        )
    
    def fuse(self, tier1_result: Dict, tier2_result: Dict) -> Dict:
        """
        èåˆä¸¤ä¸ªåˆ†ç±»å™¨çš„ç»“æœ
        
        ä½¿ç”¨ç½®ä¿¡åº¦åŠ æƒå¹³å‡ï¼š
        final_score(type) = score1(type) * w1 + score2(type) * w2
        
        Args:
            tier1_result: Tier 1ç»“æœ
            tier2_result: Tier 2ç»“æœ
            
        Returns:
            èåˆåçš„ç»“æœ:
            {
                "type": "Task|Concept|Reference",
                "confidence": 0.0-1.0,
                "scores": {"Task": 0.x, "Concept": 0.y, "Reference": 0.z},
                "reasoning": "Fusion explanation",
                "needs_review": True/False,
                "components": {
                    "tier1": {...},
                    "tier2": {...}
                }
            }
        """
        # åŠ æƒå¹³å‡åˆ†æ•°
        fused_scores = {}
        content_types = ['Task', 'Concept', 'Reference']
        
        for ctype in content_types:
            score1 = tier1_result['scores'].get(ctype, 0.0)
            score2 = tier2_result['scores'].get(ctype, 0.0)
            
            fused_score = (
                score1 * self.weights['tier1'] +
                score2 * self.weights['tier2']
            )
            fused_scores[ctype] = fused_score
        
        # å½’ä¸€åŒ–ï¼ˆç¡®ä¿æ€»å’Œä¸º1ï¼‰
        total = sum(fused_scores.values())
        if total > 0:
            fused_scores = {k: v / total for k, v in fused_scores.items()}
        
        # é€‰æ‹©æœ€é«˜åˆ†
        best_type = max(fused_scores, key=fused_scores.get)
        final_confidence = fused_scores[best_type]
        
        # ç”Ÿæˆæ¨ç†è¯´æ˜
        reasoning = self._generate_reasoning(
            best_type,
            tier1_result,
            tier2_result,
            fused_scores,
            final_confidence
        )
        
        # åˆ¤æ–­æ˜¯å¦éœ€è¦äººå·¥å®¡æ ¸
        needs_review = final_confidence < self.confidence_threshold
        
        return {
            "type": best_type,
            "confidence": final_confidence,
            "scores": fused_scores,
            "reasoning": reasoning,
            "needs_review": needs_review,
            "components": {
                "tier1": {
                    "type": tier1_result['type'],
                    "confidence": tier1_result['confidence'],
                    "matched_rules": tier1_result.get('matched_rules', [])
                },
                "tier2": {
                    "type": tier2_result['type'],
                    "confidence": tier2_result['confidence'],
                    "reasoning": tier2_result.get('reasoning', '')
                }
            }
        }
    
    def _generate_reasoning(
        self,
        final_type: str,
        tier1: Dict,
        tier2: Dict,
        fused_scores: Dict,
        final_confidence: float
    ) -> str:
        """ç”Ÿæˆèåˆæ¨ç†è¯´æ˜"""
        
        tier1_type = tier1['type']
        tier2_type = tier2['type']
        tier1_conf = tier1['confidence']
        tier2_conf = tier2['confidence']
        
        # æƒ…å†µ1: ä¸¤å±‚åˆ†ç±»å™¨å®Œå…¨ä¸€è‡´
        if tier1_type == tier2_type == final_type:
            return (
                f"Strong agreement: Both classifiers predict {final_type} "
                f"(Rules: {tier1_conf:.2f}, LLM: {tier2_conf:.2f})"
            )
        
        # æƒ…å†µ2: è§„åˆ™åˆ†ç±»å™¨ä¸»å¯¼
        elif tier1_type == final_type and tier1_conf > 0.8:
            return (
                f"Rule-based classifier dominates: {final_type} with high confidence "
                f"({tier1_conf:.2f}), LLM suggested {tier2_type} ({tier2_conf:.2f})"
            )
        
        # æƒ…å†µ3: LLMåˆ†ç±»å™¨ä¸»å¯¼
        elif tier2_type == final_type and tier2_conf > 0.8:
            return (
                f"LLM classifier dominates: {final_type} with high confidence "
                f"({tier2_conf:.2f}), Rules suggested {tier1_type} ({tier1_conf:.2f})"
            )
        
        # æƒ…å†µ4: èåˆè§£å†³å†²çª
        elif tier1_type != tier2_type:
            return (
                f"Fusion resolved conflict: Rules={tier1_type}({tier1_conf:.2f}), "
                f"LLM={tier2_type}({tier2_conf:.2f}), Final={final_type}({final_confidence:.2f})"
            )
        
        # æƒ…å†µ5: å…¶ä»–æƒ…å†µ
        else:
            return (
                f"Weighted fusion result: {final_type} "
                f"(confidence: {final_confidence:.2f})"
            )