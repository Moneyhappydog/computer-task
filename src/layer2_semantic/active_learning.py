"""
ä¸»åŠ¨å­¦ä¹ ç®¡ç†å™¨ - Tier 3
ç®¡ç†éœ€è¦äººå·¥å®¡æ ¸çš„ä½ç½®ä¿¡åº¦æ¡ˆä¾‹ï¼ŒæŒç»­æ”¹è¿›ç³»ç»Ÿ
"""
from pathlib import Path
from typing import Dict, List
import json
from datetime import datetime

# å¯¼å…¥å·¥å…·æ¨¡å—
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.utils.logger import setup_logger

logger = setup_logger('active_learning')

class ActiveLearningManager:
    """ä¸»åŠ¨å­¦ä¹ ç®¡ç†å™¨ - Tier 3åˆ†ç±»å™¨"""
    
    def __init__(self, review_dir: Path = None):
        """
        åˆå§‹åŒ–ä¸»åŠ¨å­¦ä¹ ç®¡ç†å™¨
        
        Args:
            review_dir: äººå·¥å®¡æ ¸é˜Ÿåˆ—ç›®å½•
        """
        if review_dir is None:
            review_dir = Path("data/review_queue")
        
        self.review_dir = Path(review_dir)
        self.review_dir.mkdir(parents=True, exist_ok=True)
        
        self.training_set_path = self.review_dir / "training_set.jsonl"
        self.pending_review_path = self.review_dir / "pending_review.json"
        
        logger.info(f"ğŸ“š ä¸»åŠ¨å­¦ä¹ ç®¡ç†å™¨åˆå§‹åŒ–: {review_dir}")
    
    def mark_for_review(
        self,
        chunk: Dict,
        tier1_result: Dict,
        tier2_result: Dict
    ) -> Dict:
        """
        æ ‡è®°éœ€è¦äººå·¥å®¡æ ¸çš„æ¡ˆä¾‹
        
        å½“ä¸¤å±‚åˆ†ç±»å™¨éƒ½ä¸ç¡®å®šæ—¶ï¼ˆç½®ä¿¡åº¦è¿‡ä½ï¼‰ï¼Œæäº¤ç»™äººå·¥å®¡æ ¸
        
        Args:
            chunk: æ–‡æœ¬å—
            tier1_result: Tier 1åˆ†ç±»ç»“æœ
            tier2_result: Tier 2åˆ†ç±»ç»“æœ
            
        Returns:
            å®¡æ ¸è¯·æ±‚ç»“æœ
        """
        review_item = {
            "chunk_id": chunk["id"],
            "title": chunk["title"],
            "content": chunk["content"][:500] + "...",  # ä»…ä¿å­˜å‰500å­—ç¬¦
            "timestamp": datetime.now().isoformat(),
            "tier1": {
                "type": tier1_result["type"],
                "confidence": tier1_result["confidence"],
                "matched_rules": tier1_result.get("matched_rules", [])
            },
            "tier2": {
                "type": tier2_result["type"],
                "confidence": tier2_result["confidence"],
                "reasoning": tier2_result.get("reasoning", "")
            },
            "status": "pending",
            "human_label": None
        }
        
        # åŠ è½½ç°æœ‰é˜Ÿåˆ—
        pending = self._load_pending_queue()
        pending.append(review_item)
        
        # ä¿å­˜
        with open(self.pending_review_path, 'w', encoding='utf-8') as f:
            json.dump(pending, f, indent=2, ensure_ascii=False)
        
        logger.warning(
            f"âš ï¸ å— '{chunk['title'][:30]}...' éœ€è¦äººå·¥å®¡æ ¸ "
            f"(Tier1: {tier1_result['type']}/{tier1_result['confidence']:.2f}, "
            f"Tier2: {tier2_result['type']}/{tier2_result['confidence']:.2f})"
        )
        
        return {
            "type": "needs_review",
            "confidence": 0.0,
            "scores": {"Task": 0.33, "Concept": 0.33, "Reference": 0.33},
            "reasoning": "Low confidence from both classifiers, marked for human review",
            "review_id": review_item["chunk_id"]
        }
    
    def submit_human_label(self, chunk_id: str, human_label: str):
        """
        æäº¤äººå·¥æ ‡æ³¨ç»“æœ
        
        Args:
            chunk_id: å—ID
            human_label: äººå·¥æ ‡æ³¨ç»“æœ (Task/Concept/Reference)
        """
        if human_label not in ["Task", "Concept", "Reference"]:
            raise ValueError(f"æ— æ•ˆçš„æ ‡æ³¨: {human_label}")
        
        # ä»å¾…å®¡æ ¸é˜Ÿåˆ—ä¸­æ‰¾åˆ°è¯¥é¡¹
        pending = self._load_pending_queue()
        
        found = False
        for item in pending:
            if item["chunk_id"] == chunk_id:
                item["human_label"] = human_label
                item["status"] = "reviewed"
                item["reviewed_at"] = datetime.now().isoformat()
                
                # æ·»åŠ åˆ°è®­ç»ƒé›†
                self._add_to_training_set(item)
                
                logger.info(f"âœ… äººå·¥æ ‡æ³¨å·²è®°å½•: {chunk_id} â†’ {human_label}")
                found = True
                break
        
        if not found:
            logger.error(f"âŒ æœªæ‰¾åˆ°å¾…å®¡æ ¸é¡¹: {chunk_id}")
            return
        
        # ä¿å­˜æ›´æ–°åçš„é˜Ÿåˆ—
        with open(self.pending_review_path, 'w', encoding='utf-8') as f:
            json.dump(pending, f, indent=2, ensure_ascii=False)
    
    def _load_pending_queue(self) -> List[Dict]:
        """åŠ è½½å¾…å®¡æ ¸é˜Ÿåˆ—"""
        if self.pending_review_path.exists():
            with open(self.pending_review_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        return []
    
    def _add_to_training_set(self, reviewed_item: Dict):
        """å°†å®¡æ ¸åçš„é¡¹æ·»åŠ åˆ°è®­ç»ƒé›†"""
        training_example = {
            "title": reviewed_item["title"],
            "content": reviewed_item["content"],
            "label": reviewed_item["human_label"],
            "timestamp": reviewed_item["reviewed_at"],
            "tier1_prediction": reviewed_item["tier1"]["type"],
            "tier2_prediction": reviewed_item["tier2"]["type"],
            "tier1_confidence": reviewed_item["tier1"]["confidence"],
            "tier2_confidence": reviewed_item["tier2"]["confidence"]
        }
        
        # è¿½åŠ åˆ°è®­ç»ƒé›†æ–‡ä»¶ï¼ˆJSONLæ ¼å¼ï¼Œæ¯è¡Œä¸€ä¸ªJSONï¼‰
        with open(self.training_set_path, 'a', encoding='utf-8') as f:
            f.write(json.dumps(training_example, ensure_ascii=False) + '\n')
        
        logger.info(f"ğŸ“– è®­ç»ƒé›†å·²æ›´æ–°: {self.training_set_path}")
    
    def get_pending_count(self) -> int:
        """è·å–å¾…å®¡æ ¸æ•°é‡"""
        pending = self._load_pending_queue()
        return sum(1 for item in pending if item["status"] == "pending")
    
    def get_pending_items(self) -> List[Dict]:
        """è·å–æ‰€æœ‰å¾…å®¡æ ¸é¡¹"""
        pending = self._load_pending_queue()
        return [item for item in pending if item["status"] == "pending"]
    
    def export_training_data(self, output_path: Path):
        """
        å¯¼å‡ºè®­ç»ƒæ•°æ®ä¸ºJSONæ ¼å¼
        
        Args:
            output_path: è¾“å‡ºè·¯å¾„
        """
        if not self.training_set_path.exists():
            logger.warning("âš ï¸ è®­ç»ƒé›†ä¸ºç©º")
            return
        
        training_data = []
        with open(self.training_set_path, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    training_data.append(json.loads(line))
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(training_data, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ğŸ“Š è®­ç»ƒæ•°æ®å·²å¯¼å‡º: {output_path} ({len(training_data)} æ¡)")
    
    def get_statistics(self) -> Dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        pending = self._load_pending_queue()
        
        total = len(pending)
        pending_count = sum(1 for item in pending if item["status"] == "pending")
        reviewed_count = sum(1 for item in pending if item["status"] == "reviewed")
        
        # ç»Ÿè®¡è®­ç»ƒé›†å¤§å°
        training_count = 0
        if self.training_set_path.exists():
            with open(self.training_set_path, 'r', encoding='utf-8') as f:
                training_count = sum(1 for line in f if line.strip())
        
        return {
            "total_items": total,
            "pending": pending_count,
            "reviewed": reviewed_count,
            "training_set_size": training_count
        }